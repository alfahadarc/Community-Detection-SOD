{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10113440,"sourceType":"datasetVersion","datasetId":6094392}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nimport networkx as nx\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster import hierarchy\nimport time\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:34:12.388634Z","iopub.execute_input":"2024-12-05T19:34:12.389137Z","iopub.status.idle":"2024-12-05T19:34:12.414619Z","shell.execute_reply.started":"2024-12-05T19:34:12.389094Z","shell.execute_reply":"2024-12-05T19:34:12.413326Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/dlpfc-151669/151669_qc_plot.png\n/kaggle/input/dlpfc-151669/151669_count_matrix.csv\n/kaggle/input/dlpfc-151669/151669_meta_data.csv\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"## Load metadata and count matrix from 12 samples","metadata":{}},{"cell_type":"code","source":"samples = {}\nsample_numbers = ['151507', '151508', '151509', '151510', '151669', '151670', '151671', '151672', '151673', '151674', '151675', '151676']\n\nfor i in sample_numbers[0:2]:\n    m_file = f\"/kaggle/input/dlpfc-151669/{i}_meta_data.csv\"  # Adjust for each sample's metadata file\n    c_file = f\"/kaggle/input/dlpfc-151669/{i}_count_matrix.csv\"  # Adjust for each sample's count matrix file\n    \n    # Read the CSV files for metadata and count matrix\n    m = pd.read_csv(m_file, index_col=0)\n    c = pd.read_csv(c_file, index_col=0)\n    \n    # Store both the metadata and count matrix as a list for each sample\n    samples[f'Sample {i}'] = [m, c]\n\n# m1 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_meta_data.csv\", index_col =0) \n# c1 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_count_matrix.csv\", index_col =0)\n# samples['Sample 1'] = [m1, c1]\n\n# m2 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_meta_data.csv\", index_col =0) \n# c2 = pd.read_csv(\"/kaggle/input/dlpfc-151669/151669_count_matrix.csv\", index_col =0)\n# samples['Sample 1'] = [m2, c2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T20:11:10.019961Z","iopub.execute_input":"2024-12-05T20:11:10.020476Z","iopub.status.idle":"2024-12-05T20:13:50.329175Z","shell.execute_reply.started":"2024-12-05T20:11:10.020439Z","shell.execute_reply":"2024-12-05T20:13:50.327551Z"}},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":"## FUNCTION: Loop through 12 datasets and perform PCA, aggregation, and clustering with desired parameters. Save clustering plot and metrics.","metadata":{}},{"cell_type":"code","source":"# counts = samples['sample_1'][0] # Extract count matrix\n# meta = samples['sample_1'][1] # Extract metadata\n\ndef run_samples(PCA_amount, num_hops, weight_weights, num_cluster):\n    # Empty dictionary for clustering performance metrics\n    metrics = {}\n    # Iterate over each sample in dictionary\n    for sample in samples:\n        counts = samples[sample][1] # Extract count matrix\n        meta = samples[sample][0] # Extract metadata\n    \n        # Remove the rows with NaN from both 'meta' and 'counts' DataFrames\n        nan_index = meta[meta['Layer'].isna()].index\n        meta = meta.drop(nan_index)\n        counts = counts.drop(nan_index)\n    \n        # Convert ground truth cluster assignment to categorical\n        meta['Layer'] = pd.Categorical(meta['Layer'])\n    \n        # Scale and PCA\n        scaler = StandardScaler()\n        count_scaled = scaler.fit_transform(counts)\n        pca = PCA(n_components = PCA_amount)\n        count_pca_array = pca.fit_transform(count_scaled)\n        count_pca = pd.DataFrame(count_pca_array,index=counts.index)\n    \n        # Create graph object\n        G = create_graph(sample, count_pca, meta)\n        \n        # Aggregate neighborhood features and concatenate with own features\n        node_features = torch.from_numpy(count_pca_array)\n        aggregated_features = aggregate_k_hops(G, node_features, num_hops, weight_weights)\n        own_and_aggregated_features = torch.cat((node_features, aggregated_features), dim=1)\n    \n        # Cluster\n        # kmeans = KMeans(n_clusters=5, init='random', random_state=42, n_init='auto')  # Specify the number of clusters\n        # kmeans.fit(own_and_aggregated_features)\n        # clusters = kmeans.labels_\n        # meta['Clusters'] = clusters\n\n        # Determine number of clusters using inertia elbow method, cluster using KMeans++\n        k_values = range(5, 15)\n        k , labels = best_k(k_values, own_and_aggregated_features, meta=None, figFile=f\"{sample}: Elbow Plot.png\")\n        meta['Clusters'] = labels\n    \n        # Compute metrics and save to dict\n        ari = adjusted_rand_score(meta['Layer'], clusters)\n        metrics[sample] = [labels, ari, k]\n    \n        # Generate ground truth and clustering result spatial plots (colored by cluster)\n        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n        \n        sns.scatterplot(x='X', y='Y', hue='Layer', data=meta, palette='Set1', s=10, ax=axs[0])\n        axs[0].set_title(f\"{sample}: Ground Truth\")    \n        \n        sns.scatterplot(x='X', y='Y', hue='Clusters', data=meta, palette='Set1', s=10,ax=axs[1])\n        axs[1].set_title(f\"{sample}: Clusters\")\n        \n        plt.tight_layout() # Adjust spacing between subplots\n        plt.savefig(f'/kaggle/working/{sample}: Cluster Plot.png')\n        plt.show()\n\n    # Compute average and sd of metrics (ARI)\n    metrics_matrix = np.array([value[1:] for value in metrics.values()])\n    avg_ari = np.mean(metrics_matrix[:, 0])\n    sd_ari = np.std(metrics_matrix[:, 0])\n    \n    return metrics, avg_ari, sd_ari","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T20:19:35.639846Z","iopub.execute_input":"2024-12-05T20:19:35.640376Z","iopub.status.idle":"2024-12-05T20:19:35.654892Z","shell.execute_reply.started":"2024-12-05T20:19:35.640329Z","shell.execute_reply":"2024-12-05T20:19:35.653684Z"}},"outputs":[],"execution_count":105},{"cell_type":"markdown","source":"## FUNCTION: Create graph object from spatial coordinates and save plot","metadata":{}},{"cell_type":"code","source":"def create_graph(sample_name, count_pca, meta):\n    num_spots = np.shape(count_pca)[0]  # number of spots (nodes)\n    spatial_coordinates = meta.iloc[:,0:2]\n \n    # Create graph objet\n    G = nx.Graph()\n     \n    # Add nodes with their gene expression PC data as features\n    for i in range(num_spots):\n        G.add_node(i, features=torch.tensor(count_pca.iloc[i,], dtype=torch.float32))\n     \n    # Compute pairwise Euclidean distances between spots based on their spatial coordinates\n    distance_matrix = cdist(spatial_coordinates, spatial_coordinates)\n     \n    # Define a distance threshold for creating edges based on spatial proximity\n    distance_threshold = 150\n     \n    # Add edges based on the spatial distance threshold\n    for i in range(num_spots):\n        for j in range(i + 1, num_spots):\n            if distance_matrix[i, j] < distance_threshold:\n                G.add_edge(i, j)\n    \n    # Save node/edge plot    \n    positions = {i: spatial_coordinates.iloc[i] for i in range(num_spots)}\n    plt.figure(figsize=(10, 10)) \n    nx.draw(G, pos=positions, with_labels=False, node_size=10, node_color=\"red\", font_size=12, font_weight=\"bold\")\n    plt.title(f\"{sample_name}: Node and Edge Plot\", fontsize=16)\n     \n    plt.savefig(f'/kaggle/working/{sample_name}: Node and Edge Plot.png')\n    plt.show()\n\n    return G","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:44:26.302807Z","iopub.execute_input":"2024-12-05T19:44:26.303699Z","iopub.status.idle":"2024-12-05T19:44:26.313496Z","shell.execute_reply.started":"2024-12-05T19:44:26.303647Z","shell.execute_reply":"2024-12-05T19:44:26.312219Z"}},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":"## FUNCTION: Aggregate neighborhood information from graph","metadata":{}},{"cell_type":"code","source":"def aggregate_k_hops(graph, node_features, k, weight_weights):\n    weights = generate_exponential_decay_values(k) * weight_weights\n    feature_dim = node_features.shape[1]\n    aggregated_features = torch.zeros(node_features.shape[0], feature_dim * 1)\n    \n    for node in graph.nodes():\n        current_neighbors = {node}\n        all_neighbors = set()\n        weighted_mean_sum = torch.zeros(feature_dim)\n        feature_parts = []\n        \n        for hop in range(1, k + 1):\n            # Expand to the next hop\n            next_neighbors = set()\n            for n in current_neighbors:\n                next_neighbors.update(graph.neighbors(n))\n            current_neighbors = next_neighbors - all_neighbors - {node}  # Avoid duplicates and self-loops\n            all_neighbors.update(current_neighbors)\n            \n            # Aggregate features for the current hop\n            if len(current_neighbors) > 0:\n                neighbor_features = node_features[list(current_neighbors)]\n                mean_features = neighbor_features.mean(dim=0) \n                # max_features = neighbor_features.max(dim=0).values\n            else:\n                mean_features = torch.zeros(feature_dim) \n                # max_features = torch.zeros(feature_dim)\n            \n            # feature_parts.extend([mean_features, max_features])\n            # feature_parts.extend([mean_features])\n            weighted_mean_sum += weights[hop - 1] * mean_features\n        \n        # Concatenate all aggregated features for this node\n        # aggregated_features[node] = torch.cat(feature_parts)\n        # aggregated_features[node] = torch.cat([weighted_mean_sum, max_features])\n        aggregated_features[node] = weighted_mean_sum\n    \n    return aggregated_features\n\n\ndef generate_exponential_decay_values(k, rate=.5):\n    # Generate k values that decay exponentially: e^(-rate * i)\n    indices = np.arange(1, k + 1)  # Generate indices from 1 to k\n    exp_values = np.exp(-rate * indices)  # Apply exponential decay\n    \n    # Normalize the values so that their sum equals 1\n    normalized_values = exp_values / np.sum(exp_values)\n    \n    return normalized_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:18:59.650576Z","iopub.execute_input":"2024-12-05T19:18:59.651017Z","iopub.status.idle":"2024-12-05T19:18:59.662448Z","shell.execute_reply.started":"2024-12-05T19:18:59.650953Z","shell.execute_reply":"2024-12-05T19:18:59.660861Z"}},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":"## FUNCTION: Clustering","metadata":{}},{"cell_type":"code","source":"def find_elbow_point(k_values, costs):\n    # Normalize k_values and costs to [0, 1] for comparison\n    k_values_normalized = (k_values - np.min(k_values)) / (np.max(k_values) - np.min(k_values))\n    costs_normalized = (costs - np.min(costs)) / (np.max(costs) - np.min(costs))\n    # Calculate the difference from the line connecting first and last points\n    line = np.array([k_values_normalized, costs_normalized]).T\n    start, end = line[0], line[-1]\n    distances = np.abs(np.cross(end-start, line-start) / np.linalg.norm(end-start))\n    # The index with the maximum distance is the elbow point\n    elbow_idx = np.argmax(distances)\n    return k_values[elbow_idx], elbow_idx\n\ndef best_k(k_values, own_and_aggregated_features, meta=None, figFile=\"output.png\"):\n    inertia_values = []\n    labels = []\n \n    # Loop through the range of k values\n    for i in k_values:\n        kmeans = KMeans(n_clusters=i, init='k-means++', algorithm=\"lloyd\", n_init='auto', random_state=0)\n        kmeans.fit(own_and_aggregated_features)\n        agg_labels = kmeans.labels_\n        inertia_values.append(kmeans.inertia_)\n        labels.append(agg_labels)\n        if meta is not None:\n            ari = adjusted_rand_score(meta['Layer'], agg_labels)\n            print(f\"k = {i}, Adjusted Rand Index (ARI): {ari}, inertia: {kmeans.inertia_}\")\n    # Plot the Elbow Method\n    plt.figure(figsize=(10, 5))\n    plt.plot(k_values, inertia_values, 'o-', label=\"Inertia (Elbow Method)\")\n    plt.xlabel(\"Number of Clusters (k)\")\n    plt.ylabel(\"Inertia\")\n    plt.title(\"Elbow Method\")\n    plt.legend()\n    plt.grid()\n    plt.savefig(figFile)\n    # Determine the optimal k using the Elbow Method\n    best_k, best_idx = find_elbow_point(k_values, inertia_values)\n    print(f\"The optimal k (Elbow Point) is: {best_k}\")\n    # Return the best k and the corresponding cluster labels\n    return best_k, labels[best_idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T20:19:48.942436Z","iopub.execute_input":"2024-12-05T20:19:48.942956Z","iopub.status.idle":"2024-12-05T20:19:48.954713Z","shell.execute_reply.started":"2024-12-05T20:19:48.942916Z","shell.execute_reply":"2024-12-05T20:19:48.953419Z"}},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":"## Perform hyperparameter search","metadata":{}},{"cell_type":"code","source":"# Hyperparameter search values\nhyp_PCA = [99, 90, 80]\nhyp_num_hops = [1, 2, 3, 4, 5]\nhyp_weight_weights = [.75, 1, 1.25, 1.5]\n\nt = 0\nfor PCA_amount in hyp_PCA:\n    for num_hops in hyp_num_hops:\n        for weight_weights in hyp_weight_weights:\n            t += 1\n\n            # Run samples with specified hyperparameters\n            start_time = time.time()\n            metrics, avg_ari, sd_ari = run_samples(PCA_amount, num_hops, weight_weights, num_clusters)\n            end_time = time.time()\n            execution_time = end_time - start_time    \n\n            print(t, execution_time, avg_ari, sd_ari, PCA_amount, num_hops, weight_weights)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics, avg_ari, sd_ari = run_samples(PCA_amount=80, num_hops=3, weight_weights=1.25)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(avg_ari, sd_ari)\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T19:58:27.789705Z","iopub.execute_input":"2024-12-05T19:58:27.790228Z","iopub.status.idle":"2024-12-05T19:58:27.798400Z","shell.execute_reply.started":"2024-12-05T19:58:27.790190Z","shell.execute_reply":"2024-12-05T19:58:27.796863Z"}},"outputs":[{"name":"stdout","text":"0.2502439102578752 0.0\n{'sample_1': [array([1, 4, 3, ..., 1, 4, 2], dtype=int32), 0.2502439102578752]}\n","output_type":"stream"}],"execution_count":100}]}